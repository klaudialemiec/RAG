import json
import os
from typing import Optional

import pinecone
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain.embeddings.base import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
from langchain_community.chat_models import google_palm
from langchain_pinecone import PineconeVectorStore


class RAG:
    """
    The RAG (Retrieval-Augmented Generation) class represents a model that combines retrieval-based and
    generation-based approaches for question answering.

    Args:
        index_name (str): The name of the index used for storing and retrieving documents.
        embedding_model (Embeddings): The embedding model used for encoding text into vectors.
        data_path (str): The path to the data directory containing the documents.
        text_splitter (Optional[TextSplitter]): The text splitter used for splitting documents into chunks.
            If not provided, a default RecursiveCharacterTextSplitter will be used.

    Attributes:
        pc_client (PineconeClient): The Pinecone client used for interacting with the Pinecone service.
        index_name (str): The name of the index used for storing and retrieving documents.
        embedding_model (Embeddings): The embedding model used for encoding text into vectors.
        text_splitter (TextSplitter): The text splitter used for splitting documents into chunks.
        vectorstore (PineconeVectorStore): The vector store used for storing and retrieving document vectors.
        qa (RetrievalQA): The RAG model for question answering.

    Methods:
        prepare_vector_store(data_path: str) -> None:
            Prepares the vector store for the RAG model by adding documents from the specified data path.
        ask(question: str) -> str:
            Queries the RAG model with a question and returns the answer.

    """

    def __init__(
        self,
        index_name: str,
        embedding_model: Embeddings,
        data_path: str,
        text_splitter: Optional[TextSplitter] = None,
    ) -> None:
        load_dotenv()
        self.pc_client = pinecone.Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

        self.index_name = index_name
        self.embedding_model = embedding_model
        self.text_splitter = (
            text_splitter
            if text_splitter
            else RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        )

        # check if index exists
        indexes = self.pc_client.list_indexes().names()
        # if not, create it
        if index_name not in indexes:
            embedding_dim = embedding_model.client.get_sentence_embedding_dimension()
            metric = embedding_model.client.similarity_fn_name

            self.pc_client.create_index(
                name=index_name,
                dimension=embedding_dim,
                metric=metric,
                spec=pinecone.ServerlessSpec(cloud="aws", region="us-east-1"),
            )

        self.vectorstore = PineconeVectorStore(
            index_name=index_name, embedding=embedding_model
        )
        # add sources data to the vector store
        self.prepare_vector_store(data_path)

        # use the vector store as a retriever
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        # create the RAG model using the retriever and PaLM model
        self.qa = RetrievalQA.from_chain_type(
            llm=google_palm.ChatGooglePalm(google_api_key=os.getenv("GOOGLE_API_KEY")),
            chain_type="stuff",
            retriever=retriever,
        )

    def prepare_vector_store(self, data_path: str) -> None:
        """Prepare vector store for the RAG model.

        Args:
            data_path (str): Path to the data.
        """

        documents = []
        for file in os.listdir(data_path):
            with open(f"{data_path}/{file}", "r") as f:
                data = json.load(f)
                documents.append(
                    Document(
                        page_content=data["content"],
                        metadata={"title": data["title"], "url": data["url"]},
                    )
                )

        docs_splittted = self.text_splitter.split_documents(documents)
        self.vectorstore.add_documents(docs_splittted)

    def ask(self, question: str) -> str:
        """Query the RAG model.

        Args:
            question (str): The question to ask the RAG model.

        Returns:
            str: The answer generated by the RAG model.
        """
        return self.qa.invoke(question)["result"]
